# Prometheus Alerting Rules for Crypto Screener
groups:
  - name: crypto-screener-alerts
    interval: 30s
    rules:
      # Service Down Alerts
      - alert: ServiceDown
        expr: up{job=~"data-collector|metrics-calculator|alert-engine|api-gateway"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Service {{ $labels.job }} is down"
          description: "{{ $labels.job }} has been down for more than 2 minutes"

      # High Error Rate
      - alert: HighErrorRate
        expr: rate(api_gateway_http_requests_total{status=~"5.."}[5m]) > 0.05
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High error rate on API Gateway"
          description: "Error rate is {{ $value | humanizePercentage }} over the last 5 minutes"

      # Database Connection Issues
      - alert: DatabaseConnectionPoolExhausted
        expr: database_connection_pool_size > 9
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "Database connection pool near capacity"
          description: "{{ $labels.service }} connection pool at {{ $value }} connections (max 10)"

      # WebSocket Connection Issues
      - alert: LowWebSocketConnections
        expr: data_collector_websocket_connections < 40
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "WebSocket connections below threshold"
          description: "Only {{ $value }} WebSocket connections active (expected ~43)"

      # NATS Message Lag
      - alert: NATSMessageLag
        expr: rate(nats_messages_received_total[5m]) < rate(nats_messages_published_total[5m]) * 0.9
        for: 3m
        labels:
          severity: warning
        annotations:
          summary: "NATS message processing lag detected"
          description: "Message consumption is 10% slower than production"

      # High Memory Usage
      - alert: HighMemoryUsage
        expr: container_memory_usage_bytes / container_spec_memory_limit_bytes > 0.9
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage on {{ $labels.pod }}"
          description: "Memory usage is {{ $value | humanizePercentage }} of limit"

      # Slow Database Inserts
      - alert: SlowDatabaseInserts
        expr: metrics_calculator_db_insert_duration_seconds_avg > 0.1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Slow database inserts detected"
          description: "Average insert time is {{ $value }}s (target <50ms)"

      # Alert Evaluation Lag
      - alert: SlowAlertEvaluation
        expr: alert_engine_evaluation_duration_seconds_avg > 0.01
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Alert evaluation taking too long"
          description: "Average evaluation time is {{ $value }}s (target <1ms)"

      # Webhook Failures
      - alert: HighWebhookFailureRate
        expr: rate(alert_engine_webhooks_failed_total[5m]) > 0.1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High webhook failure rate"
          description: "{{ $value | humanizePercentage }} of webhooks are failing"

      # Disk Space
      - alert: LowDiskSpace
        expr: kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes < 0.2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Low disk space on {{ $labels.persistentvolumeclaim }}"
          description: "Only {{ $value | humanizePercentage }} disk space remaining"

      # Pod Restart
      - alert: PodRestarting
        expr: rate(kube_pod_container_status_restarts_total[15m]) > 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Pod {{ $labels.pod }} is restarting"
          description: "Pod has restarted {{ $value }} times in the last 15 minutes"
